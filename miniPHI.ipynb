{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "miniPHI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJn/IiLZCu6hU7Bb+Wf20I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andresni/miniPHI/blob/master/miniPHI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCsEFiJIpVsn",
        "colab_type": "text"
      },
      "source": [
        "# Approximations and heuristics of PHI\n",
        "This is a notebook for generating logic gate networks, calculating PHI on them, as well as approximations and heuristics of PHI.\n",
        "\n",
        "PHI, $\\theta$, is a measure of integrated information, from the Integrated Information Theory of Consciousness (IIT) by Guilio Tononi (Oizumi et al. 2014), and reflects a system's (here a logic gate network) cause effect power over itself. In other words, $\\theta$ reflects how much a network in a given state causally restricts its past and future state space, above and beyond how much its parts causally restricts their past and future state spaces.\n",
        "\n",
        "The issue with $\\theta$ as currently formulated is that it's not feasible to calculate $\\theta$ for any system with more than a dozen nodes. Thus, it is of interest to either find heuristics that capture similar underlying aspects as $\\theta$, and approximations to $\\theta$ that reduce some of the computational complexity of the original calculations.\n",
        "\n",
        "In this notebook, we will provide code that generates networks and for each network automatically calculates $\\theta$ as well as a selection of other properties.\n",
        "\n",
        "See the paper by [Nilsen, Juel, and Marshall (2019)](https://www.mdpi.com/1099-4300/21/5/525) for more information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-7H3up05e62",
        "colab_type": "text"
      },
      "source": [
        "### Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obZD7AxapSge",
        "colab_type": "code",
        "outputId": "6f613b19-e450-41da-a55a-f8833b9ef1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        }
      },
      "source": [
        "%pip install pyphi\n",
        "%pip install git+https://github.com/andresni/pyconscious.git\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import time\n",
        "import itertools\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import scipy.io as sio\n",
        "import scipy as spi\n",
        "import pyphi\n",
        "import pyconscious as pc"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyphi in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.6/dist-packages (from pyphi) (3.13)\n",
            "Requirement already satisfied: tqdm>=4.20.0 in /usr/local/lib/python3.6/dist-packages (from pyphi) (4.38.0)\n",
            "Requirement already satisfied: pymongo>=2.7.1 in /usr/local/lib/python3.6/dist-packages (from pyphi) (3.10.1)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from pyphi) (1.4.1)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pyphi) (4.4.2)\n",
            "Requirement already satisfied: tblib>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from pyphi) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from pyphi) (0.14.1)\n",
            "Requirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from pyphi) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from pyphi) (1.18.2)\n",
            "Requirement already satisfied: psutil>=2.1.1 in /usr/local/lib/python3.6/dist-packages (from pyphi) (5.4.8)\n",
            "Requirement already satisfied: redis>=2.10.5 in /usr/local/lib/python3.6/dist-packages (from pyphi) (3.4.1)\n",
            "Collecting git+https://github.com/andresni/pyconscious.git\n",
            "  Cloning https://github.com/andresni/pyconscious.git to /tmp/pip-req-build-o5g0a4jm\n",
            "  Running command git clone -q https://github.com/andresni/pyconscious.git /tmp/pip-req-build-o5g0a4jm\n",
            "Requirement already satisfied (use --upgrade to upgrade): pyconscious==0.1 from git+https://github.com/andresni/pyconscious.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyconscious==0.1) (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyconscious==0.1) (1.4.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from pyconscious==0.1) (0.10.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels->pyconscious==0.1) (0.25.3)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->pyconscious==0.1) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->pyconscious==0.1) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->pyconscious==0.1) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.0->statsmodels->pyconscious==0.1) (1.12.0)\n",
            "Building wheels for collected packages: pyconscious\n",
            "  Building wheel for pyconscious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyconscious: filename=pyconscious-0.1-cp36-none-any.whl size=7986 sha256=96ae16b4fc04eac2d95c57f0ac3072533931ac14f19efed3b8a3b1f24e626cce\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rogntt2h/wheels/23/ad/ac/47ef26f546b837efed439d7099e44a822bde018abce384d259\n",
            "Successfully built pyconscious\n",
            "\n",
            "Welcome to PyPhi!\n",
            "\n",
            "If you use PyPhi in your research, please cite the paper:\n",
            "\n",
            "  Mayner WGP, Marshall W, Albantakis L, Findlay G, Marchman R, Tononi G.\n",
            "  (2018). PyPhi: A toolbox for integrated information theory.\n",
            "  PLOS Computational Biology 14(7): e1006343.\n",
            "  https://doi.org/10.1371/journal.pcbi.1006343\n",
            "\n",
            "Documentation is available online (or with the built-in `help()` function):\n",
            "  https://pyphi.readthedocs.io\n",
            "\n",
            "To report issues, please use the issue tracker on the GitHub repository:\n",
            "  https://github.com/wmayner/pyphi\n",
            "\n",
            "For general discussion, you are welcome to join the pyphi-users group:\n",
            "  https://groups.google.com/forum/#!forum/pyphi-users\n",
            "\n",
            "To suppress this message, either:\n",
            "  - Set `WELCOME_OFF: true` in your `pyphi_config.yml` file, or\n",
            "  - Set the environment variable PYPHI_WELCOME_OFF to any value in your shell:\n",
            "        export PYPHI_WELCOME_OFF='yes'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U78vNpK1Ljhs",
        "colab_type": "text"
      },
      "source": [
        "### Functions for generating networks\n",
        "Here we generate networks $M$ with the property\n",
        "\n",
        "$$M_{i,j} =\n",
        " \\begin{pmatrix}\n",
        "  0 & w_{1,2} & \\cdots & w_{1,j} \\\\\n",
        "  w_{2,1} & 0 & \\cdots & w_{2,j} \\\\\n",
        "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "  w_{i,1} & w_{i,2} & \\cdots & 0\n",
        " \\end{pmatrix}$$\n",
        " \n",
        " where $w_{i, j}=[-1,0,1]$ is the weight between nodes $i$ and $j$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BKtnqzvLmAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def net_gen_next(Params):\n",
        "    networks = []\n",
        "            \n",
        "    for size in range(Params[\"minsize\"], Params[\"maxsize\"] + 1):\n",
        "        temp_net = []\n",
        "        for x in range(0, Params[\"nrnetworks\"]):\n",
        "            temp = np.zeros([size,size])\n",
        "            \n",
        "            conn_prob = np.random.uniform(0.2, 1.0)\n",
        "            inh_prob = np.random.uniform(0.0, 0.8)\n",
        "            \n",
        "            for xx in range(0,size):\n",
        "                for yy in range(0,size):\n",
        "                    if not xx==yy:\n",
        "                        if np.random.rand() < conn_prob:\n",
        "                            temp[xx][yy] = np.random.normal(1, 0)\n",
        "                            if np.random.rand() < inh_prob:\n",
        "                                temp[xx][yy] = np.random.normal(-1, 0)\n",
        "            \n",
        "            temp2 = list(itertools.permutations([it for it in range(size)]))\n",
        "            temp3 = [temp[:, list(it)][list(it)] for it in temp2]\n",
        "            \n",
        "            yes = 0\n",
        "            for ii in temp3:\n",
        "                for zz in temp_net:    \n",
        "                    if len(np.unique([ii, zz], axis=0)) < 2:\n",
        "                        yes = 1\n",
        "                        break\n",
        "            if not yes:\n",
        "              temp_net.append(copy.copy(temp))\n",
        "                    \n",
        "        for xx in temp_net:\n",
        "            networks.append(xx)\n",
        "    \n",
        "    return np.array(networks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zftli4B6NRf7",
        "colab_type": "text"
      },
      "source": [
        "### Functions for generating activity\n",
        "Here are functions that generate activity $A_t$, where \n",
        "\n",
        "$$A_t = \\left[\\begin{array}{c}a^{t-1}_1\\\\a^{t-1}_2\\\\\\vdots\\\\a^{t-1}_i\\end{array}\\right]\\cdot M \\cdot C=\\left[\\begin{array}{c}a^t_1\\\\a^t_2\\\\\\vdots\\\\a^t_i\\end{array}\\right]$$\n",
        "\n",
        "with $a^t_i=[0,1]$, $t$ is timestep, $i$ is network element, and $A_0$ is set to one of $2^i$ possible binary states. $a^t_i=1$ if $a^t_i\\geq 1$ and $0$ otherwise. $C$ is a noise parameter (for connections only).\n",
        "\n",
        "### Noise\n",
        "There are three kinds of possible noise.\n",
        "\n",
        "1. Connection noise\n",
        "\n",
        "$C$ is an array of length $i$ with random values from a gaussian distribution with $\\mu=0, \\sigma=s$. \n",
        "\n",
        "2. Node noise\n",
        "\n",
        "$a^t_i=1$ has a probability of flipping to $a^t_i=0$ with $P=p$, and the opposite with equal probability.\n",
        "\n",
        "3. Embedded noise\n",
        "\n",
        "The network $M$ is embedded in a much larger network that is \"unknowkn\" to us. I.e. $M_{small}$ is a subset of $M_{large}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFmSRlLEPpN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLOLI(nodes): # Creates an ordered list that iterates through all possible permutations of a binary array.\n",
        "    LOLI = np.zeros((int(2**nodes), int(nodes)))\n",
        "    i = 1\n",
        "    for z in range(0, len(LOLI[0, :])):\n",
        "        for x in range(i, len(LOLI[:, 0])):\n",
        "            if LOLI[x - i, z] == 1:\n",
        "                LOLI[x, z] = 0\n",
        "            else:\n",
        "                LOLI[x, z] = 1\n",
        "        i = i * 2       \n",
        "    return LOLI\n",
        "\n",
        "def makeStateByNode(spm,LOLI): # Convert activity to state by node probability matrix\n",
        "\n",
        "    sbn=np.zeros((len(LOLI),len(LOLI[0])+1))\n",
        "    a = int(len(spm[0][0]) - len(LOLI[0]))\n",
        "    \n",
        "    for x in range(len(spm)):\n",
        "      for i in range(len(spm[x])-1):\n",
        "        b=spm[x][i][:-a] if a > 0 else spm[x][i]\n",
        "        c=spm[x][i+1][:-a] if a > 0 else spm[x][i+1]\n",
        "        indexa=LOLI.index(list(b))\n",
        "        c=np.append(c,1)\n",
        "        sbn[indexa]=np.add(sbn[indexa],c)\n",
        "        \n",
        "    for i in range(0,len(sbn)):\n",
        "        for q in range(0,len(sbn[i])-1):\n",
        "            sbn[i,q]=sbn[i,q]/sbn[i][-1] if sbn[i][-1] > 0 else 0\n",
        "            sbn[i,q]=np.round(sbn[i,q],4)\n",
        "    \n",
        "    return sbn[:,0:-1]\n",
        "\n",
        "def makeStateByState(spm,LOLI): # Convert activity to state by state probability matrix\n",
        "\n",
        "    sbn=np.zeros((len(LOLI),len(LOLI)))\n",
        "    a = int(len(spm[0][0]) - len(LOLI[0]))\n",
        "    \n",
        "    for x in range(len(spm)):\n",
        "      for i in range(len(spm[x])-1):  \n",
        "        b=spm[x][i][:-a] if a > 0 else spm[x][i]\n",
        "        c=spm[x][i+1][:-a] if a > 0 else spm[x][i+1]\n",
        "        indexa=LOLI.index(list(b))\n",
        "        indexb=LOLI.index(list(c))\n",
        "        sbn[indexa,indexb]+=1\n",
        "        \n",
        "    for i in range(0,len(sbn)):\n",
        "        s=np.sum(sbn[i])\n",
        "        for q in range(0,len(sbn[i])):\n",
        "            sbn[i,q]=sbn[i,q]/s if s > 0 else 0\n",
        "            sbn[i,q]=np.round(sbn[i,q],4)\n",
        "    \n",
        "    return sbn\n",
        "\n",
        "def gen_transition_prob_matrices_state(results): # Create transition probability matrix (state by state)\n",
        "   \n",
        "    tpm = []    \n",
        "    for i in range(len(results[\"partial_activity\"])):\n",
        "        tpm.append(copy.deepcopy(makeStateByState(results[\"partial_activity\"][i],results[\"LOLI\"][i])))\n",
        "    return {\"tpms\":copy.deepcopy(tpm)}\n",
        "\n",
        "def gen_transition_prob_matrices_node(results): # Create transition probability matrix (state by node)\n",
        "    \n",
        "    tpm = []\n",
        "    for i in range(len(results[\"partial_activity\"])):\n",
        "        tpm.append(copy.deepcopy(makeStateByNode(results[\"partial_activity\"][i],results[\"LOLI\"][i])))\n",
        "    return {\"tpmn\":copy.deepcopy(tpm)}\n",
        "\n",
        "def activity(results,Params): # Generate activity given a network\n",
        "    \n",
        "    all_net_act = []\n",
        "    all_net_pact = []\n",
        "    all_net_LOLI = []\n",
        "    \n",
        "    for i in results[\"networks\"]:\n",
        "        partial = []\n",
        "        LOLI = createLOLI(len(i))\n",
        "\n",
        "        replace = True if Params[\"states\"] > len(LOLI) else False\n",
        "        states = np.random.choice(list(range(len(LOLI))), Params[\"states\"], replace = replace)\n",
        "        states = list(states) * Params[\"steps\"]\n",
        "\n",
        "        for L in states:\n",
        "            state_block = [LOLI[L].tolist()]\n",
        "            for rounds in range(Params[\"samples\"]):\n",
        "                temp = np.zeros(len(i))\n",
        "                for x in range(len(i)):\n",
        "                    if LOLI[L][x] > 0:\n",
        "                        for y in range(len(i)):\n",
        "                          noise = np.random.uniform(-Params[\"noiseLVL\"], Params[\"noiseLVL\"]) if Params[\"noise\"] == \"con\" else 0\n",
        "                          temp[y] = temp[y] + i[x][y] + np.random.uniform(noise, noise)\n",
        "                for x in range(len(i)):\n",
        "                  temp[x] = 1 if temp[x] >= 1 else 0\n",
        "                  if Params[\"noise\"] == \"node\":\n",
        "                    temp[x] = [1, 0][int(temp[x])] if np.random.rand() < Params[\"noiseLVL\"] else [0, 1][int(temp[x])]\n",
        "                state_block.append(temp.tolist())\n",
        "            partial.append(copy.deepcopy(state_block))\n",
        "        \n",
        "        all_net_pact.append(copy.deepcopy(partial))\n",
        "        all_net_LOLI.append(LOLI.tolist())\n",
        "        \n",
        "    return {\"partial_activity\": all_net_pact, \"LOLI\": all_net_LOLI}\n",
        "\n",
        "def net_gen_noise(net,Params):\n",
        "  result = []\n",
        "  if Params[\"noiseLVL\"] > 0:\n",
        "    conn_net = np.zeros((Params[\"noiseLVL\"]+len(net),Params[\"noiseLVL\"]+len(net)))\n",
        "    u, c = np.unique(net, return_counts = True)\n",
        "    nrcons = np.sum(np.abs(net)) / np.sum(net**0)\n",
        "    nrinh = c[0] / np.sum(np.abs(net)) if -1 in u else 0\n",
        "  \n",
        "    for x in range(len(conn_net)):\n",
        "      for y in range(len(conn_net)):\n",
        "        conn_net[x,y] = 1 if np.random.rand() < nrcons else 0\n",
        "        conn_net[x,y] = -1 if np.random.rand() < nrinh and conn_net[x,y] == 1 else conn_net[x,y]\n",
        "    conn_net[:-Params[\"noiseLVL\"],:-Params[\"noiseLVL\"]] = net\n",
        "  else:\n",
        "    conn_net = net\n",
        "  \n",
        "  LOLI = statsPlots.createLOLI(len(net))\n",
        "  \n",
        "  replace = True if Params[\"states\"] > len(LOLI) else False\n",
        "  states = np.random.choice(list(range(len(LOLI))),Params[\"states\"],replace=replace)\n",
        "  states = list(states) * Params[\"steps\"]\n",
        "  for L in states:\n",
        "    S=list(LOLI[L])  \n",
        "    state_block = [list(np.random.randint(0, 2, size=(len(conn_net))))]\n",
        "    if Params[\"noiseLVL\"] > 0:\n",
        "      state_block[-1][:-Params[\"noiseLVL\"]] = copy.deepcopy(S)\n",
        "    else:\n",
        "      state_block[-1] = copy.deepcopy(S)\n",
        "    S=copy.deepcopy(state_block[-1])\n",
        "    for rounds in range(Params[\"samples\"]):\n",
        "      temp = np.zeros(len(conn_net))\n",
        "      for x in range(len(conn_net)):\n",
        "        for y in range(len(conn_net)):\n",
        "          temp[y]=temp[y]+(S[x]*conn_net[x][y])\n",
        "      for x in range(len(net)):\n",
        "          temp[x] = 1 if temp[x] >= 1 else 0\n",
        "      S = list(temp)\n",
        "      state_block.append(list(temp))\n",
        "        \n",
        "    result.append(copy.deepcopy(state_block))\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOcQNSzZaS35",
        "colab_type": "text"
      },
      "source": [
        "### Analysis tools\n",
        "\n",
        "Now we just need to generate some analysis functions.\n",
        "\n",
        "**PHI 3.0 calculations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQV0q2Xcaqaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def phi(struct,net,config_opt):\n",
        "    \n",
        "    result = {}\n",
        "\n",
        "    whole_sia = []\n",
        "    whole_phi = []\n",
        "    whole_concepts = []\n",
        "    statenr = []  \n",
        "    all_sia =[]\n",
        "    mc_sia = []\n",
        "    mc_phi = []\n",
        "    mc_size = []\n",
        "    mc_whole = []\n",
        "    mc_concepts = []\n",
        "    mc_mc = []\n",
        "    time = []\n",
        "  \n",
        "    pyphi.config.ASSUME_CUTS_CANNOT_CREATE_NEW_CONCEPTS = config_opt[\"no_new_concepts\"]\n",
        "    pyphi.config.CUT_ONE_APPROXIMATION = config_opt[\"cut_one_aprox\"]\n",
        "    pyphi.config.MAXIMUM_CACHE_MEMORY_PERCENTAGE = config_opt[\"max_cache\"]\n",
        "    pyphi.config.CACHE_POTENTIAL_PURVIEWS = config_opt[\"cache_pot_purw\"]\n",
        "    pyphi.config.CLEAR_SUBSYSTEM_CACHES_AFTER_COMPUTING_SIA = config_opt[\"clear_sys_cache\"]\n",
        "    pyphi.config.CACHE_SIAS = True\n",
        "    pyphi.config.CACHE_REPERTOIRES = True\n",
        "    \n",
        "    pyphi.config.PARTITION_TYPE = config_opt[\"part_type\"]\n",
        "    pyphi.config.PROGRESS_BARS=False\n",
        "    pyphi.config.PARALLEL_CUT_EVALUATION=False\n",
        "    pyphi.config.PARALLEL_COMPLEX_EVALUATION=True\n",
        "    pyphi.config.NUMBER_OF_CORES=16\n",
        "    pyphi.config.LOG_STDOUT_LEVEL=\"ERROR\"\n",
        "    pyphi.config.LOG_FILE_LEVEL=\"ERROR\"\n",
        "    pyphi.config.log()\n",
        "\n",
        "    tpms = struct[\"tpmn\"][net]\n",
        "    LOLI = struct[\"LOLI\"][net]\n",
        "    conmat = np.sign(struct[\"networks\"][net])\n",
        "\n",
        "    labels = [chr(97+i) for i in range(len(conmat))]\n",
        "    network=pyphi.Network(tpms,cm=np.abs(conmat),node_labels=labels)\n",
        "\n",
        "    temp = [calcphi(LOLI[s],network,labels,s,len(LOLI)) for s in range(len(LOLI))]\n",
        "   \n",
        "    for xs in temp:\n",
        "        whole_sia.append(copy.deepcopy(xs[\"whole_sia\"]))\n",
        "        whole_phi.append(copy.deepcopy(xs[\"whole_phi\"]))\n",
        "        whole_concepts.append(copy.deepcopy(xs[\"whole_concepts\"]))\n",
        "        statenr.append(copy.deepcopy(xs[\"statenr\"]))\n",
        "        mc_sia.append(copy.deepcopy(xs[\"mc_sia\"]))\n",
        "        mc_phi.append(copy.deepcopy(xs[\"mc_phi\"]))\n",
        "        mc_size.append(copy.deepcopy(xs[\"mc_size\"]))\n",
        "        mc_whole.append(copy.deepcopy(xs[\"mc_whole\"]))\n",
        "        mc_concepts.append(copy.deepcopy(xs[\"mc_concepts\"]))\n",
        "        mc_mc.append(copy.deepcopy(xs[\"mc_mc\"]))\n",
        "        all_sia.append(copy.deepcopy(xs[\"all_sia\"]))\n",
        "        time.append(copy.deepcopy(xs[\"time\"]))\n",
        "    \n",
        "    return {\"mc_phi\":mc_phi}\n",
        "\n",
        "def calcphi(state,network,labels,s,tot):\n",
        "    result = {\"statenr\":s,\"whole_sia\":np.NaN,\"whole_phi\":np.NaN,\"whole_concepts\":np.NaN,\"all_sia\":np.NaN,\"time\":np.NaN}\n",
        "    result.update({\"mc_sia\":np.NaN,\"mc_phi\":np.NaN,\"mc_size\":np.NaN,\"mc_whole\":np.NaN,\"mc_concepts\":np.NaN,\"mc_mc\":np.NaN})\n",
        "    phitime = time.time()\n",
        "    cstate=np.array(state).astype(int)    \n",
        "\n",
        "    try:\n",
        "        whole_sia = pyphi.compute.network.all_complexes(network,cstate)\n",
        "                      \n",
        "        for i in range(len(whole_sia)):\n",
        "            if (whole_sia[i].phi > result[\"whole_phi\"] and len(whole_sia[i].subsystem)== len(labels)) or np.isnan(result[\"whole_phi\"]):\n",
        "                result[\"whole_phi\"] = whole_sia[i].phi\n",
        "                result[\"whole_concepts\"] = len(whole_sia[i].ces)\n",
        "                result[\"whole_sia\"] = np.NaN#whole_sia[i]\n",
        "            if whole_sia[i].phi > result[\"mc_phi\"] or np.isnan(result[\"mc_phi\"]) or (whole_sia[i].phi == result[\"mc_phi\"] and len(whole_sia[i].ces) > result[\"mc_size\"]):\n",
        "                result[\"mc_phi\"] = whole_sia[i].phi\n",
        "                result[\"mc_concepts\"] = len(whole_sia[i].ces)\n",
        "                result[\"mc_sia\"] = np.NaN#whole_sia[i]\n",
        "                result[\"mc_size\"] = len(whole_sia[i].subsystem)\n",
        "                result[\"mc_mc\"] = whole_sia[i].subsystem.node_indices\n",
        "                result[\"mc_whole\"] = 1 if result[\"mc_size\"] == len(labels) else 0\n",
        "        result[\"all_sia\"]=len(whole_sia)\n",
        "        phitime=time.time()-phitime\n",
        "        result[\"time\"]=copy.deepcopy(phitime)\n",
        "\n",
        "        return result\n",
        "    \n",
        "    except Exception as e:\n",
        "        if \"cannot be reached\" in str(e):\n",
        "            return result\n",
        "        else:\n",
        "            print(\"Failed big-time\")\n",
        "            print(str(e))\n",
        "            crash=10/0\n",
        "\n",
        "def complexity(results,n):\n",
        "  lz = []\n",
        "  for i in results[\"partial_activity\"][n]:\n",
        "    b = pc.functions.concatenate(np.array(i).astype(int),concat=\"space\")\n",
        "    c = pc.functions.compress(b)\n",
        "    d = pc.functions.compress(\"\".join(random.sample([str(random.randint(0,1)) for x in range(len(b))],len(b))))\n",
        "    lz.append(c/d)\n",
        "  return lz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cNEb_yCKLdq",
        "colab_type": "text"
      },
      "source": [
        "## Ready to rumble\n",
        "\n",
        "**First we define parameters...**\n",
        "\n",
        "\"*Steps*\" which refers to how many states should a network visit following a given initialization. This will be normalized if multiple network sizes are used so that the information content of different network sizes are the same (i.e. fewer nodes = more steps).\n",
        "\n",
        "\"*Samples*\" refers to how many times should a given initialization be repeated.\n",
        "\n",
        "\"*States*\" refers to how many initializations should be performed\n",
        "\n",
        "\"*noiseLVL*\" refers to how much noise should be present in the network activity.\n",
        "\n",
        "\"*noise*\" refers to type of noise, with options \"con\" (fuzzy connections), \"node\" (fuzzy nodes), \"net\" (embedded network, i.e. hidden background nodes influencing network)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG1zv5bRJ7Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Params = {\"nrnetworks\": 100,  # generate how many networks of each size\n",
        "          \"minsize\": 3,  # min node count\n",
        "          \"maxsize\": 3,  # max node count\n",
        "          \"steps\": 50,  # number of steps per network\n",
        "          \"samples\": 50,  # number of samples per state\n",
        "          \"states\": 20,  # number of states to start in\n",
        "          \"noiseLVL\": 0.00,  # plus minus noise\n",
        "          \"noise\": \"none\",  # kind of noise (none, con, node, net)\n",
        "          }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqdW61YidpeG",
        "colab_type": "text"
      },
      "source": [
        "**Then we generate networks and associated activity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDK6sNqwds1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = {\"networks\": net_gen_next(Params)}\n",
        "\n",
        "results.update(activity(results,Params))\n",
        "results.update(gen_transition_prob_matrices_state(results))\n",
        "results.update(gen_transition_prob_matrices_node(results))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW4U3_aAgISF",
        "colab_type": "text"
      },
      "source": [
        "**And now we can finally calculate PHI3.0 for these networks**\n",
        "\n",
        "Be warned though, this code is not optimized for GPUs, supercomputers, etc. Depending on your resources, networks with 6 nodes can take up to an hour, 7 nodes can take up to a day. So don't go running too many big nets. The config parameters instruct how PHI should be calculated (if using approximations for certain steps of the calculation, partitioning type, etc.). Also consider running with local resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSe4YdMMizoS",
        "colab_type": "code",
        "outputId": "c8e2b3d2-6128-4c0e-e310-adb2164c82e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "pf = pd.DataFrame(columns=[\"Net\",\"Nodes\",\"PhiMax\",\"PhiMean\",\"LZmax\",\"LZmean\"])\n",
        "\n",
        "# Config options - \n",
        "config_opt = {\"no_new_concepts\": False, # No new concepts approximation\n",
        "              \"cut_one_aprox\": False, # Cut one approximation\n",
        "              \"part_type\": \"BI\", # Partition type (BI, TRI, FULL) - approximation\n",
        "              \"par_comp_eval\": True, # Parallel computation (False if par. comp. higher up)\n",
        "              \"max_cache\": 50, # Cache size\n",
        "              \"cache_pot_purw\": True, # Faster but higher mem. use\n",
        "              \"clear_sys_cache\": False, # Slower but lower mem. use\n",
        "              }\n",
        "\n",
        "for i in range(len(results[\"networks\"])):\n",
        "  a = []\n",
        "  t = phi(results,i,config_opt)\n",
        "  a.extend([np.nanmean(t[\"mc_phi\"]),np.nanmax(t[\"mc_phi\"])])\n",
        "  t = complexity(results,i)\n",
        "  a.extend([np.nanmean(t),np.nanmax(t)])\n",
        "  pf = pf.append({\"Net\":i, \"Nodes\":len(results[\"networks\"][i]), \"PhiMax\":a[1], \"PhiMean\":a[0],\"LZmax\":a[3], \"LZmean\":a[2]}, ignore_index=True)\n",
        "print(pf)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Net  Nodes    PhiMax   PhiMean     LZmax    LZmean\n",
            "0    0.0    3.0  0.000000  0.000000  0.488889  0.411004\n",
            "1    1.0    3.0  0.629633  0.304290  0.600000  0.493162\n",
            "2    2.0    3.0  0.256945  0.093254  0.600000  0.507084\n",
            "3    3.0    3.0  0.112144  0.078699  0.577778  0.483834\n",
            "4    4.0    3.0  0.468339  0.308280  0.600000  0.490305\n",
            "..   ...    ...       ...       ...       ...       ...\n",
            "56  56.0    3.0  0.000000  0.000000  0.600000  0.474495\n",
            "57  57.0    3.0  0.000000  0.000000  0.600000  0.526020\n",
            "58  58.0    3.0  0.000000  0.000000  0.590909  0.484841\n",
            "59  59.0    3.0  0.000000  0.000000  0.577778  0.471894\n",
            "60  60.0    3.0  0.382579  0.261990  0.577778  0.514799\n",
            "\n",
            "[61 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7ykWbWt-dUS",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "The above is a simple demonstration. To test your own measures, add them as a function. The item \"partial_activity\" reflects $A_t$ initialized from each possible state (out of $2^i$), and \"networks\" reflect $M$. As output from the $\\theta$ calculations is only the $\\theta$ of the maximally irreducible cause-effect structure. However, other alternatives such as $\\theta_{whole}$ can be output by changing the function.\n",
        "\n",
        "For questions or otherwise, please contact **sevenius.nilsen@gmail.com**"
      ]
    }
  ]
}